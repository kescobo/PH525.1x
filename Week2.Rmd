---
title: "Week2"
author: "Kevin Bonham"
date: "2May, 2016"
output: html_document
---
# Random Variables and Probability Distributions

## Video notes
Statistical inference - error values, p values etc

```{r}
library(dplyr)
dat = read.csv("dagdata/inst/extdata/femaleMiceWeights.csv")

controls = filter(dat, Diet=="chow") %>% select(Bodyweight) %>% unlist
treatment = filter(dat, Diet=="hf") %>% select(Bodyweight) %>% unlist

mean(controls)
mean(treatment)
mean(treatment) - mean(controls) 
```

Because 12 mice selected for controls and for hf diet were randomly selected, their mean weights are random variables. 

As demo, we can use data from Jax that has entire population distribution

```{r}
population = read.csv("dagdata/inst/extdata/femaleControlsPopulation.csv") %>% unlist

mean( sample(population, 12) ) # 22.5475
# Each repeat gives different answer
mean( sample(population, 12) ) # 23.2025
```

Random variables fluctuate depending on the sample taken. What is the range of fluctuation?

### Assessment
```{r}
avg = mean(population)

set.seed(1)
abs( avg - mean(sample(population, 5)))

set.seed(5)
abs( avg - mean(sample(population, 5)))
```

## Null distributions

Null hypothesis is that the hf diet had no effect. Can simmulate with `population` to see how likely it is that the difference in weight of the control and treatment mice would arise just due to sampling. 

```{r}
obs = mean(treatment) - mean(controls) 

s_control = sample(population, 12)
s_treatment = sample(population, 12)
mean(s_control) - mean(s_treatment) # -1.49

# each time we run this, we get a different value again
s_control = sample(population, 12)
s_treatment = sample(population, 12)
mean(s_control) - mean(s_treatment) # -0.1

# ... run again
s_control = sample(population, 12)
s_treatment = sample(population, 12)
mean(s_control) - mean(s_treatment) # 3.06
```

Null distribution is the entire range of outcomes that could under the null hypothesis. To simulate (for loops not the best option apparently, but...)

```{r}
n = 10000
nulls = vector("numeric", n)
for(i in 1:n){
  s_control = sample(population, 12)
  s_treatment = sample(population, 12)
  nulls[i] = mean(s_control) - mean(s_treatment)
}

max(nulls)
min(nulls)

hist(nulls)
```

So, given that our observed difference was ~3 grams, how often would we observe this under the null hypothesis?

```{r}
nulls > obs # returns a boolean vector

# since TRUE evals to 1 and FALSE to 0...
mean( nulls > obs ) # 0.013

# maybe hf diet would make skinnier mice - we want to know how often the difference would be bigger in both directions
mean( abs(nulls) > obs ) # 0.0259
```

That's the p value. But normally we don't have the entire population to check against... so how do we infer?

### Assessment
```{r}
# 1
set.seed(1)
n = 1000
nulls = vector("numeric", n)
for(i in 1:n){
  s_control = sample(population, 5)
  nulls[i] = mean(s_control)
}

mean(abs(nulls - mean(population)) > 1) # 0.498

# 2
set.seed(1)
n = 10000
nulls = vector("numeric", n)
for(i in 1:n){
  s_control = sample(population, 5)
  nulls[i] = mean(s_control)
}

mean(abs(nulls - mean(population)) > 1) # 0.4976

# 3
set.seed(1)
n = 1000
nulls = vector("numeric", n)
for(i in 1:n){
  s_control = sample(population, 50)
  nulls[i] = mean(s_control)
}

mean(abs(nulls - mean(population)) > 1) # 0.019
```

Increasing sample size decreases deviation from sample mean.

## Probability distributions

Given the heights of all males in population, we can say what's the probability that a given man is below a given height with the function F(a) = Pr(Height < a). Then we can say what's the probability that a man is between two heights (a and b) with F(b) - F(a)

### Assessment
```{r}
library(gapminder)
data(gapminder)
head(gapminder)

x = filter(gapminder, year==1952) 
hist(x$lifeExp)
```

*The empirical cumulative distribution function (or empirical cdf or empirical distribution function) is the function F(a) for any a, which tells you the proportion of the values which are less than or equal to a.*

can do this with `mean(x <= a)`, or use `ecdf()`

```{r}
le = x$lifeExp
mean(le <= 40)

mean(le <= 60) - mean(le <= 40)

plot(ecdf(le)) # plots proportion under given value `q`
# eg.
f = ecdf(le)
f(60) - f(40)

# same as:
prop = function(q) {
  mean(le <= q)
}
prop(60) - prop(40)

qs = seq(from=min(le), to=max(le), length=20)

props = sapply(qs, prop)
plot(qs, props)
"ecdf_manual.png"
# annonymous function version
props = sapply(qs, function(q) mean(le <= q))
```

![picture](graphs/ecdf_manual.png)

# Central Limit Theorem

If distribution is normal, mean (μ) and standard deviation (σ) define the entire distribution. If data set is normal, 95% of values are between 2σ of the mean etc, 99% within 2.5σ etc. 

For dataset M, with mean = μ and standard dev = σ (using python notation for list comprehension :-/) 
Variance = σ^2 = 1 / length(M) * sum([x - μ for x in M])

*Quantile-quantile (qq) plot*: How well does your data fit the normal distribution? Checks actual value for each quantile (1%, 2%... etc) against expected value in normal distribution. If perfect fit, will cluster around y = x

*Standard Units*: number of standard deviations from the mean (Z)

### p-set 1 and notes
```{r eval=FALSE}

n = 1000
means5 = vector("numeric", n)
for(i in 1:n){
  s_control = sample(population, 5)
  means5[i] = mean(s_control)
}

n = 1000
means50 = vector("numeric", n)
for(i in 1:n){
  s_control = sample(population, 50)
  means50[i] = mean(s_control)
}

hist(means5)
hist(means50)
```

![5](graphs/means5.png) ![50](graphs/means50.png)

```{r}
f = ecdf(means50) # genderates empirical cumulative distribution function
f(25)-f(23) # 0.98

g = pnorm(c(23, 25), mean=23.9, sd=0.43) # [1] 0.01817354 0.99473831
g[2] - g[1] # 0.9765648
```

## Statistical inferences

*Notation*: If the population = x, samples are $X_1$, $X_2$ etc. $μ_x$ = population mean (as opposed to $\bar{X}$

$\bar{X}$ is a random variable. CLT helps us determine how far away $\bar{X}$is likely to be from $μ_x$

```{r eval=FALSE}
dat = read.csv("dagdata/inst/extdata/mice_pheno.csv")
dat = na.omit(dat)

head(dat)

x = filter(dat, Diet=="chow" & Sex=="M") %>% select(Bodyweight) %>% unlist
mean(x) # 30.96381

library(rafalib)
popsd(x) # 4.420501

set.seed(1)
X = sample(x, 25)
mean(X) # 32.0956


y = filter(dat, Diet=="hf" & Sex=="M") %>% select(Bodyweight) %>% unlist
mean(y) # 34.84793
popsd(y) # 5.574609

set.seed(1)
Y = sample(y, 25)
mean(Y) # 34.768

abs( (mean(y) - mean(x)) - (mean(Y) - mean(X))) # 1.211716


w = filter(dat, Diet=="chow" & Sex=="F") %>% select(Bodyweight) %>% unlist
mean(x)

set.seed(1)
W = sample(w, 25)

z = filter(dat, Diet=="hf" & Sex=="F") %>% select(Bodyweight) %>% unlist

set.seed(1)
Z = sample(z, 25)

abs( (mean(z) - mean(w)) - (mean(Z) - mean(W))) # 0.7364828
```

## CLT
[textbook chapter](http://genomicsclass.github.io/book/pages/clt_and_t-distribution.html)

$$ \frac{\bar{Y} - \mu}{\sigma_Y/\sqrt{N}} $$

Remember, %\sigma^2% is variance.
**Note**: Standard error ~= $\sigma$, just for a sample instead of population

### Problems and notes
```{r}
pnorm(1) - pnorm(-1) # 0.6826895
pnorm(2) - pnorm(-2) # 0.9544997
pnorm(3) - pnorm(-3) # 0.9973002

cmm = mean(x)
cmsd = popsd(x)

mean(x <= (cmm + cmsd)) - mean(x <= (cmm - cmsd)) # 0.6950673
mean(x <= (cmm + 2*cmsd)) - mean(x <= (cmm - 2*cmsd)) # 0.9461883
mean(x <= (cmm + 3*cmsd)) - mean(x <= (cmm - 3*cmsd)) # 0.9910314



avgs <- replicate(10000, mean( sample(x, 25)))
mypar(1,2)
hist(avgs)
qqnorm(avgs)
qqline(avgs)

mean(avgs) # 30.9556
popsd(avgs) # 0.8367952

```
### Problems 2


```{r}
# Dice simulation
n = 100
x = sample(1:6, n, replace=TRUE)

mean(x==6) # 0.14
```

We want to roll n dice 10,000 times and keep these proportions. This random variable (proportion of 6s) has mean `p=1/6` and variance `p*(1-p)/n`. So according to CLT `z = (mean(x==6) - p) / sqrt(p*(1-p)/n)` should be normal with mean 0 and SD 1. Set the seed to 1, then use `replicate` to perform the simulation, and report what proportion of times z was larger than 2 in absolute value (CLT says it should be about 0.05).

```{r}
set.seed(1)
?replicate

n = 100
p = 1/6
z = (mean(x==6) - p) / sqrt(p*(1-p)/n)

reps = replicate(10000, (mean(sample(1:6, n, replace=TRUE)==6) - p) / sqrt(p*(1-p)/n))
mean(abs(reps) > 2) # 0.0424

f = function(p, n) {
  reps = replicate(10000, (mean(sample(1:1/p, n, replace=TRUE)==1/p) - p) / sqrt(p*(1-p)/n))
  mean(abs(reps) > 2)
}

f(0.5, 5) # 0.0595
f(0.5, 30) # 0.0395
f(0.01, 30) # 0.0343
f(0.01, 100) # 0.0343

```


```{r}
dat <- read.csv("dagdata/inst/extdata/femaleMiceWeights.csv")
X <- filter(dat, Diet=="chow") %>% select(Bodyweight) %>% unlist
Y <- filter(dat, Diet=="hf") %>% select(Bodyweight) %>% unlist

m = mean(X) # 23.81333
s = sd(X) # 3.022541

# approximate the probability that our estimate mean(X) is off by more than 2 grams from μX.

1 - (pnorm(2.292179)-pnorm(-2.292179)) #  0.02189532
```

```{r}
sqrt(var(Y)/12+var(X)/12) # 1.469867

(mean(Y)-mean(X)) / sqrt(var(Y)/12+var(X)/12) # 2.055174
1 - pt(3,df=3)
1 - pt(3,df=15)

1 - (pnorm(2.055174) - pnorm(-2.055174)) # 0.0398622

t.test(X, Y)
```

